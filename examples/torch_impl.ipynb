{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ez/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from lm_human_preferences.env.rlhf_env import RLHFEnv\n",
    "from lm_human_preferences.data.base import QueryData\n",
    "from transformers import AutoTokenizer\n",
    "from lm_human_preferences.lm.reward import RewardModel\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "gym.envs.register(\n",
    "    id='RLHFEnv-v0',\n",
    "    entry_point='lm_human_preferences.env.rlhf_env:RLHFEnv',\n",
    "    kwargs={\n",
    "        'ref_model_name': 'openai-community/gpt2',\n",
    "        'reward_model': RewardModel.from_pretrained('../models/reward_model'),\n",
    "        'dataset': QueryData.from_openai_format(\n",
    "            AutoTokenizer.from_pretrained('openai-community/gpt2'),\n",
    "            '../data/descriptiveness_offline_5k'\n",
    "        ),\n",
    "        'kl_coef': 0.01,\n",
    "        'max_generation': 64,\n",
    "        'device': device,\n",
    "        'seed': 42\n",
    "    }\n",
    ")\n",
    "\n",
    "ENV_NAME = 'RLHFEnv-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    Buffer to store experiences collected during rollouts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.logprobs = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the buffer.\"\"\"\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "        self.logprobs.clear()\n",
    "        self.state_values.clear()\n",
    "        self.is_terminals.clear()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.states)\n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple:\n",
    "        self.__assert_all_equal_size()\n",
    "        return self.states[idx], self.actions[idx], self.rewards[idx], self.logprobs[idx], self.state_values[idx], self.is_terminals[idx]\n",
    "    \n",
    "    def shuffle(self) -> None:\n",
    "        self.__assert_all_equal_size()\n",
    "        \n",
    "        indices = list(range(len(self.states)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        self.states = [self.states[i] for i in indices]\n",
    "        self.actions = [self.actions[i] for i in indices]\n",
    "        self.rewards = [self.rewards[i] for i in indices]\n",
    "        self.logprobs = [self.logprobs[i] for i in indices]\n",
    "        self.state_values = [self.state_values[i] for i in indices]\n",
    "        self.is_terminals = [self.is_terminals[i] for i in indices]\n",
    "\n",
    "    def sample(self, n: int) -> tuple:\n",
    "        self.__assert_all_equal_size()\n",
    "        \n",
    "        indices = random.sample(list(range(len(self.states))), n)\n",
    "\n",
    "        states = [self.states[i] for i in indices]\n",
    "        actions = [self.actions[i] for i in indices]\n",
    "        rewards = [self.rewards[i] for i in indices]\n",
    "        logprobs = [self.logprobs[i] for i in indices]\n",
    "        state_values = [self.state_values[i] for i in indices]\n",
    "        is_terminals = [self.is_terminals[i] for i in indices]\n",
    "        \n",
    "        return states, actions, rewards, logprobs, state_values, is_terminals\n",
    "    \n",
    "    def __assert_all_equal_size(self):\n",
    "        assert len(self.states) == len(self.actions) == len(self.rewards) == len(self.logprobs) == len(self.state_values) == len(self.is_terminals), \\\n",
    "            \"Buffer states, actions, rewards, logprobs, state_values, and is_terminals must have the same length got {}\".format(\n",
    "                len(self.states), \n",
    "                len(self.actions), \n",
    "                len(self.rewards), \n",
    "                len(self.logprobs), \n",
    "                len(self.state_values), \n",
    "                len(self.is_terminals)\n",
    "            )\n",
    "\n",
    "class LMActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor network for continuous or discrete action spaces.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, pad_token_id: int):\n",
    "        super(LMActorNetwork, self).__init__()\n",
    "        \n",
    "        self.lm_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.lm_model.config.pad_token_id = pad_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network. Return action probs\"\"\"\n",
    "        assert state.ndim in [1, 2], \"State must be a 1D or 2D tensor got : {}\".format(state)\n",
    "        single = (state.ndim == 1)\n",
    "        if single:\n",
    "            state = state.unsqueeze(dim=0) # [1, seq_len]\n",
    "        \n",
    "        input_ids = state\n",
    "        attention_mask = input_ids != self.pad_token_id\n",
    "        logits = self.lm_model(input_ids=input_ids, attention_mask=attention_mask).logits # [batch_size, seq_len, vocab_size]\n",
    "        next_token_logits = logits[:, -1, :]  # [batch_size, vocab_size]\n",
    "        if single:\n",
    "            return next_token_logits[0] # [vocab_size]\n",
    "        return next_token_logits # [batch_size, vocab_size]\n",
    "\n",
    "class LMCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic network that estimates the value of a state.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, pad_token_id: int):\n",
    "        super(LMCriticNetwork, self).__init__()\n",
    "        \n",
    "        self.lm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=1\n",
    "        )\n",
    "        self.lm_model.config.pad_token_id = pad_token_id\n",
    "        \n",
    "        self.pad_token_id = pad_token_id\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network. Return tensor([1])\"\"\"\n",
    "        assert state.ndim in [1, 2], \"State must be a 1D or 2D tensor got : {}\".format(state)\n",
    "        single = (state.ndim == 1)\n",
    "        if single:\n",
    "            state = state.unsqueeze(dim=0) # [1, seq_len]\n",
    "        \n",
    "        input_ids = state\n",
    "        attention_mask = input_ids != self.pad_token_id\n",
    "        logits = self.lm_model(input_ids=input_ids, attention_mask=attention_mask).logits # [batch_size, 1]\n",
    "        logits = logits.squeeze(-1)\n",
    "        \n",
    "        if single:\n",
    "            return logits[0] # 0-D tensor\n",
    "        return logits # [batch_size]\n",
    "    \n",
    "class MLPActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor network using a Multi-Layer Perceptron for continuous or discrete action spaces.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super(MLPActorNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network. Return action probs\"\"\"\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.fc3(x) # [batch_size, output_dim]\n",
    "        return logits\n",
    "\n",
    "class MLPCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic network using a Multi-Layer Perceptron to estimate the value of a state.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super(MLPCriticNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network. Return state value\"\"\"\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class PPOConfig:\n",
    "    # TODO: Investigate the following hyperparameter clip_coef, vf_coef, ent_coef, norm_adv, max_grad_norm\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr_actor: float = 3e-4,\n",
    "        lr_critic: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        update_epochs: int = 10,\n",
    "        eps_clip: float = 0.2,\n",
    "        minibatch_size: int = 8,\n",
    "        pad_obs: bool = False,\n",
    "        int_obs: bool = False,\n",
    "    ):\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_critic = lr_critic\n",
    "        self.gamma = gamma\n",
    "        self.update_epochs = update_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.pad_obs = pad_obs\n",
    "        self.int_obs = int_obs\n",
    "        \n",
    "class PPO:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm with clipped objective.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        actor: nn.Module,\n",
    "        critic: nn.Module,\n",
    "        config: PPOConfig, \n",
    "        device: str = 'cuda',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the PPO agent.\n",
    "        \n",
    "        Args:\n",
    "            config: PPOConfig, \n",
    "            device: str = 'cuda',\n",
    "        ):\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.gamma = config.gamma\n",
    "        self.eps_clip = config.eps_clip\n",
    "        self.update_epochs = config.update_epochs\n",
    "        self.minibatch_size = config.minibatch_size\n",
    "        self.lr_actor = config.lr_actor\n",
    "        self.lr_critic = config.lr_critic\n",
    "        self.pad_obs = config.pad_obs\n",
    "        self.int_obs = config.int_obs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "        \n",
    "        self.actor = actor.to(device)\n",
    "        self.critic = critic.to(device)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.lr_critic)\n",
    "        \n",
    "        self.actor_old = copy.deepcopy(self.actor)\n",
    "        self.actor_old.requires_grad_(False)\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Select an action based on the current policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state observation\n",
    "            \n",
    "        Returns:\n",
    "            Selected action\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # TODO: Dynamically determine the dtype based on the observation space\n",
    "            state = torch.tensor(state, dtype=torch.int32 if self.int_obs else torch.float32).to(device)            \n",
    "            action_logits = self.actor_old(state)\n",
    "            action_dist = Categorical(logits=action_logits)\n",
    "            \n",
    "            action = action_dist.sample()\n",
    "            action_logprob = action_dist.log_prob(action)\n",
    "            state_val = self.critic(state)\n",
    "            \n",
    "            # Store experience in buffer\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "            \n",
    "            return action.item()\n",
    "    \n",
    "    def evaluate(self, states: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions given states using the current policy.\n",
    "        \n",
    "        Args:\n",
    "            states: Batch of states\n",
    "            actions: Batch of actions\n",
    "            \n",
    "        Returns:\n",
    "            log_probs, state_values, dist_entropy\n",
    "        \"\"\"\n",
    "        action_logits = self.actor(states)\n",
    "        action_dist = Categorical(logits=action_logits)\n",
    "        \n",
    "        action_log_probs = action_dist.log_prob(actions)\n",
    "        dist_entropy = action_dist.entropy().mean()\n",
    "        \n",
    "        state_values = self.critic(states)\n",
    "        \n",
    "        return action_log_probs, state_values, dist_entropy\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Update policy and value networks using PPO algorithm.\n",
    "        \"\"\"\n",
    "        # Calculate discounted rewards and advantages\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        \n",
    "        # Compute GAE\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        _, old_states, old_actions, old_logprobs, old_state_values = self._to_tensor(\n",
    "            self.buffer.rewards, \n",
    "            self.buffer.states, \n",
    "            self.buffer.actions, \n",
    "            self.buffer.logprobs, \n",
    "            self.buffer.state_values\n",
    "        )\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "        # Calculate and normalize advantages\n",
    "        advantages = rewards - old_state_values\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-7)\n",
    "        \n",
    "        T = rewards.size(0)\n",
    "        for name, tensor in [\n",
    "            (\"rewards\", rewards),\n",
    "            (\"old_actions\", old_actions),\n",
    "            (\"old_logprobs\", old_logprobs),\n",
    "            (\"old_state_values\", old_state_values)\n",
    "        ]:\n",
    "            assert tensor.dim() == 1, (\n",
    "                f\"{name} must be 1-D of length {T}, but got shape {tuple(tensor.shape)}\"\n",
    "            )\n",
    "            assert tensor.size(0) == T, (\n",
    "                f\"{name} length {tensor.size(0)} != expected {T}\"\n",
    "            )\n",
    "        assert advantages.dim() == 1 and advantages.size(0) == T, (\n",
    "            f\"advantages must be 1-D of length {T}, got {tuple(advantages.shape)}\"\n",
    "        )\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for k in range(self.update_epochs):\n",
    "            indexes = torch.randperm(len(rewards))\n",
    "            if self.minibatch_size == -1:\n",
    "                minibatch_size = len(rewards)\n",
    "            else:\n",
    "                minibatch_size = self.minibatch_size\n",
    "                \n",
    "            for i in range(0, len(rewards), minibatch_size):\n",
    "                batch_indexes = indexes[i:i + minibatch_size]\n",
    "                b_old_states = old_states[batch_indexes]\n",
    "                b_old_actions = old_actions[batch_indexes]\n",
    "                b_old_logprobs = old_logprobs[batch_indexes]\n",
    "                b_rewards = rewards[batch_indexes]\n",
    "                b_advantages = advantages[batch_indexes]\n",
    "                # b_old_states = old_states\n",
    "                # b_old_actions = old_actions\n",
    "                # b_old_logprobs = old_logprobs\n",
    "                # b_rewards = rewards\n",
    "                # b_advantages = advantages\n",
    "\n",
    "                # Evaluate old actions and values\n",
    "                b_logprobs, b_state_values, dist_entropy = self.evaluate(b_old_states, b_old_actions)\n",
    "                b_state_values = b_state_values.squeeze(-1)\n",
    "                \n",
    "                for name, tensor in [\n",
    "                    (\"b_logprobs\", b_logprobs),\n",
    "                    (\"b_state_values\", b_state_values)\n",
    "                ]:\n",
    "                    assert tensor.dim() == 1 and tensor.size(0) == minibatch_size or tensor.size(0) == len(rewards) % minibatch_size, (\n",
    "                        f\"{name} must be 1-D of length {minibatch_size} or {len(rewards) % minibatch_size}, got {tuple(tensor.shape)}\"\n",
    "                    )\n",
    "                \n",
    "                # Calculate ratios for importance sampling\n",
    "                ratios = torch.exp(b_logprobs - b_old_logprobs)\n",
    "                \n",
    "                # Calculate surrogate losses\n",
    "                surr1 = ratios * b_advantages\n",
    "                surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * b_advantages\n",
    "                \n",
    "                # PPO clipped objective with value function loss and entropy bonus\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = self.mse_loss(b_state_values, b_rewards)\n",
    "                entropy_loss = -0.01 * dist_entropy  # Entropy bonus for exploration\n",
    "                \n",
    "                # Combined loss\n",
    "                loss = actor_loss + 0.5 * critic_loss + entropy_loss\n",
    "                \n",
    "                # Take gradient step\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                self.critic_optimizer.step()\n",
    "        \n",
    "        # Update old policy\n",
    "        self.actor_old.load_state_dict(self.actor.state_dict())\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    def _to_tensor(self, rewards: List[float], states: List[np.ndarray], actions: List[np.ndarray], logprobs: List[np.ndarray], state_values: List[np.ndarray]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        if self.pad_obs:\n",
    "            # Import pad_sequence if needed for padding observations\n",
    "            from torch.nn.utils.rnn import pad_sequence\n",
    "            old_states = pad_sequence(states, batch_first=True, padding_value=actor.pad_token_id, padding_side='left').to(device)\n",
    "        else:\n",
    "            old_states = torch.stack(states, dim=0).to(device)\n",
    "        old_actions = torch.stack(actions, dim=0).detach().to(device)\n",
    "        old_logprobs = torch.stack(logprobs, dim=0).detach().to(device)\n",
    "        old_state_values = torch.stack(state_values, dim=0).squeeze().detach().to(device)\n",
    "        return rewards, old_states, old_actions, old_logprobs, old_state_values\n",
    "    \n",
    "    def save(self, checkpoint_path: str):\n",
    "        \"\"\"\n",
    "        Save model parameters.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to save the model\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'actor_old': self.actor_old.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "    \n",
    "    def load(self, checkpoint_path: str):\n",
    "        \"\"\"\n",
    "        Load model parameters.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to load the model from\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.actor_old.load_state_dict(checkpoint['actor_old'])\n",
    "\n",
    "def train(env_name: str, actor: nn.Module, critic: nn.Module, ppo_config: PPOConfig, max_episodes: int = 1000, max_timesteps: int = 1000, \n",
    "        update_interval: int = 100, save_interval: int = 100, callback: Optional[Callable] = None,\n",
    "        log_interval: int = 10, device='cuda') -> List[float]:\n",
    "    \"\"\"\n",
    "    Train a PPO agent in a given environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Name of the gym environment\n",
    "        max_episodes: Maximum number of episodes for training\n",
    "        max_timesteps: Maximum timesteps in one episode\n",
    "        update_interval: Update policy every n episodes\n",
    "        save_interval: Save model in the interval\n",
    "    \n",
    "    Returns:\n",
    "        List of average rewards per episode\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # Initialize PPO agent\n",
    "    ppo_agent = PPO(\n",
    "        actor,\n",
    "        critic,\n",
    "        ppo_config,\n",
    "        device,\n",
    "    )\n",
    "    \n",
    "    # Logging variables\n",
    "    running_reward = 0\n",
    "    time_step = 0\n",
    "    \n",
    "    # Training loop\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(1, max_episodes+1):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in range(1, max_timesteps+1):\n",
    "            # Select action\n",
    "            action = ppo_agent.select_action(state)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Store experience in buffer\n",
    "            ppo_agent.buffer.rewards.append(reward)\n",
    "            ppo_agent.buffer.is_terminals.append(done)\n",
    "            \n",
    "            time_step += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "        episode_rewards.append(running_reward)\n",
    "\n",
    "        if episode % log_interval == 0:\n",
    "            if callback is not None:\n",
    "                callback(state, action, reward, done, truncated, episode, running_reward)\n",
    "            print(f\"Episode {episode}, Running Reward: {running_reward:.2f}\")\n",
    "\n",
    "        # Update if its time\n",
    "        if episode % update_interval == 0:\n",
    "            # Make sure we have enough data in the buffer before updating\n",
    "            if len(ppo_agent.buffer.states) > 0:  # Only update if buffer contains experiences\n",
    "                ppo_agent.update()\n",
    "            else:\n",
    "                print(f\"Warning: Episode {episode} - Buffer is empty, skipping update\")\n",
    "        \n",
    "        # Save model\n",
    "        if episode % save_interval == 0:\n",
    "            ppo_agent.save(f\"./PPO_{env_name}_{episode}.pth\")\n",
    "    \n",
    "    env.close()\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "actor = LMActorNetwork(\n",
    "    model_name=\"openai-community/gpt2\",\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "critic = LMCriticNetwork(\n",
    "    model_name=\"openai-community/gpt2\",\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "[STATE]:\n",
      "  It was a strip of silver foil, carefully folded and twisted around her finger.\n",
      "\n",
      "She refused to think of what her sister would say about it, and remembered instead how she'd felt when Bobby put it there.\n",
      "\n",
      "Marry me, he'd said, on bended knee.\n",
      "\n",
      "She preferred William, hanging in that, like the butterflies in meu take on wings, now disquieting me.\n",
      "\n",
      "It wasn't like we wouldn't share this experience over and over again, we're all in this day and age.\n",
      "\n",
      "I'm not here to judge and reprimand,\n",
      "[ACTION]:\n",
      " ,\n",
      "[REWARD]:\n",
      " -0.0038928186893463085\n",
      "[DONE]:\n",
      " False\n",
      "[TRUNCATED]:\n",
      " True\n",
      "====================\n",
      "Episode 5, Running Reward: 12.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      1\u001b[39m ppo_config = PPOConfig(\n\u001b[32m      2\u001b[39m     lr_actor=\u001b[32m3e-4\u001b[39m,\n\u001b[32m      3\u001b[39m     lr_critic=\u001b[32m1e-3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     int_obs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m callback = (\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m state, action, reward, done, truncated, episode, running_reward: (\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m20\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     )\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m rewards_discrete = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mENV_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mppo_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mppo_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 330\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env_name, actor, critic, ppo_config, max_episodes, max_timesteps, update_interval, save_interval, callback, log_interval, device)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode % update_interval == \u001b[32m0\u001b[39m:\n\u001b[32m    328\u001b[39m     \u001b[38;5;66;03m# Make sure we have enough data in the buffer before updating\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ppo_agent.buffer.states) > \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# Only update if buffer contains experiences\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m         \u001b[43mppo_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Buffer is empty, skipping update\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mPPO.update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    178\u001b[39m b_advantages = advantages[batch_indexes]\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# b_old_states = old_states\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# b_old_actions = old_actions\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# b_old_logprobs = old_logprobs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m \n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Evaluate old actions and values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m b_logprobs, b_state_values, dist_entropy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_old_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_old_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m b_state_values = b_state_values.squeeze(-\u001b[32m1\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    190\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mb_logprobs\u001b[39m\u001b[33m\"\u001b[39m, b_logprobs),\n\u001b[32m    191\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mb_state_values\u001b[39m\u001b[33m\"\u001b[39m, b_state_values)\n\u001b[32m    192\u001b[39m ]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mPPO.evaluate\u001b[39m\u001b[34m(self, states, actions)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, states: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n\u001b[32m     97\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03m    Evaluate actions given states using the current policy.\u001b[39;00m\n\u001b[32m     99\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \u001b[33;03m        log_probs, state_values, dist_entropy\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     action_logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     action_dist = Categorical(logits=action_logits)\n\u001b[32m    110\u001b[39m     action_log_probs = action_dist.log_prob(actions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mLMActorNetwork.forward\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    100\u001b[39m input_ids = state\n\u001b[32m    101\u001b[39m attention_mask = input_ids != \u001b[38;5;28mself\u001b[39m.pad_token_id\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m.logits \u001b[38;5;66;03m# [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[32m    103\u001b[39m next_token_logits = logits[:, -\u001b[32m1\u001b[39m, :]  \u001b[38;5;66;03m# [batch_size, vocab_size]\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m single:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    910\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    911\u001b[39m         block.\u001b[34m__call__\u001b[39m,\n\u001b[32m    912\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    919\u001b[39m         output_attentions,\n\u001b[32m    920\u001b[39m     )\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ppo_config = PPOConfig(\n",
    "    lr_actor=3e-4,\n",
    "    lr_critic=1e-3,\n",
    "    gamma=0.99,\n",
    "    eps_clip=0.2,\n",
    "    update_epochs=10,\n",
    "    minibatch_size=-1,\n",
    "    pad_obs=True,\n",
    "    int_obs=True,\n",
    ")\n",
    "callback = (\n",
    "    lambda state, action, reward, done, truncated, episode, running_reward: (\n",
    "        print(\"=\"*20),\n",
    "        print('[STATE]:\\n', tokenizer.decode(state)),\n",
    "        print('[ACTION]:\\n', tokenizer.decode(action)),\n",
    "        print('[REWARD]:\\n', reward),\n",
    "        print('[DONE]:\\n', done),\n",
    "        print('[TRUNCATED]:\\n', truncated),\n",
    "        print(\"=\"*20)\n",
    "    )\n",
    ")\n",
    "rewards_discrete = train(\n",
    "    env_name=ENV_NAME,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    ppo_config=ppo_config,\n",
    "    max_episodes=1000, \n",
    "    max_timesteps=1000, \n",
    "    update_interval=5, \n",
    "    save_interval=10,\n",
    "    callback=callback,\n",
    "    log_interval=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Running Reward: 24.80\n",
      "Episode 200, Running Reward: 28.86\n",
      "Episode 300, Running Reward: 31.29\n",
      "Episode 400, Running Reward: 49.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     10\u001b[39m critic = MLPCriticNetwork(\n\u001b[32m     11\u001b[39m     env.observation_space.shape[\u001b[32m0\u001b[39m],\n\u001b[32m     12\u001b[39m     \u001b[32m64\u001b[39m,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m ppo_config = PPOConfig(\n\u001b[32m     16\u001b[39m     lr_actor=\u001b[32m3e-4\u001b[39m,\n\u001b[32m     17\u001b[39m     lr_critic=\u001b[32m1e-3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     minibatch_size=\u001b[32m512\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m rewards_discrete = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mppo_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mppo_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 330\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env_name, actor, critic, ppo_config, max_episodes, max_timesteps, update_interval, save_interval, callback, log_interval, device)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode % update_interval == \u001b[32m0\u001b[39m:\n\u001b[32m    328\u001b[39m     \u001b[38;5;66;03m# Make sure we have enough data in the buffer before updating\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ppo_agent.buffer.states) > \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# Only update if buffer contains experiences\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m         \u001b[43mppo_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Buffer is empty, skipping update\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 215\u001b[39m, in \u001b[36mPPO.update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m.actor_optimizer.zero_grad()\n\u001b[32m    214\u001b[39m \u001b[38;5;28mself\u001b[39m.critic_optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;28mself\u001b[39m.actor_optimizer.step()\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.critic_optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "actor = MLPActorNetwork(\n",
    "    env.observation_space.shape[0],\n",
    "    64,\n",
    "    env.action_space.n,\n",
    ")\n",
    "critic = MLPCriticNetwork(\n",
    "    env.observation_space.shape[0],\n",
    "    64,\n",
    ")\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    lr_actor=3e-4,\n",
    "    lr_critic=1e-3,\n",
    "    gamma=0.99,\n",
    "    eps_clip=0.2,\n",
    "    update_epochs=10,\n",
    "    minibatch_size=512\n",
    ")\n",
    "rewards_discrete = train(\n",
    "    env_name=env_name,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    ppo_config=ppo_config,\n",
    "    max_episodes=4_000, \n",
    "    max_timesteps=1000, \n",
    "    update_interval=100, \n",
    "    save_interval=1000,\n",
    "    log_interval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Avg. Reward: 18.61\n",
      "Episode: 200, Avg. Reward: 18.00\n",
      "Episode: 300, Avg. Reward: 23.36\n",
      "Episode: 400, Avg. Reward: 22.36\n",
      "Episode: 500, Avg. Reward: 26.66\n",
      "Episode: 600, Avg. Reward: 27.47\n",
      "Episode: 700, Avg. Reward: 23.99\n",
      "Episode: 800, Avg. Reward: 28.97\n",
      "Episode: 900, Avg. Reward: 26.57\n",
      "Episode: 1000, Avg. Reward: 29.87\n",
      "Episode: 1100, Avg. Reward: 34.25\n",
      "Episode: 1200, Avg. Reward: 33.61\n",
      "Episode: 1300, Avg. Reward: 41.06\n",
      "Episode: 1400, Avg. Reward: 35.69\n",
      "Episode: 1500, Avg. Reward: 39.38\n",
      "Episode: 1600, Avg. Reward: 50.39\n",
      "Episode: 1700, Avg. Reward: 52.65\n",
      "Episode: 1800, Avg. Reward: 66.88\n",
      "Episode: 1900, Avg. Reward: 119.23\n",
      "Episode: 2000, Avg. Reward: 131.64\n",
      "Episode: 2100, Avg. Reward: 200.44\n",
      "Episode: 2200, Avg. Reward: 252.04\n",
      "Episode: 2300, Avg. Reward: 385.38\n",
      "Episode: 2400, Avg. Reward: 361.64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 470\u001b[39m\n\u001b[32m    467\u001b[39m     plt.savefig(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle.replace(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    468\u001b[39m     plt.show()\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m rewards_discrete = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCartPole-v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_continuous_action_space\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[38;5;66;03m# plot_learning_curve(rewards_discrete, \"CartPole-v1 Learning Curve\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 419\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env_name, max_episodes, max_timesteps, update_timestep, log_interval, save_interval, has_continuous_action_space, action_std_init)\u001b[39m\n\u001b[32m    415\u001b[39m episode_reward = \u001b[32m0\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_timesteps+\u001b[32m1\u001b[39m):\n\u001b[32m    418\u001b[39m     \u001b[38;5;66;03m# Select action\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     action = \u001b[43mppo_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m     \u001b[38;5;66;03m# Take action in environment\u001b[39;00m\n\u001b[32m    422\u001b[39m     next_state, reward, done, truncated, _ = env.step(action)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 225\u001b[39m, in \u001b[36mPPO.select_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    222\u001b[39m     action_dist = Categorical(action_probs)\n\u001b[32m    224\u001b[39m action = action_dist.sample()\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m action_logprob = \u001b[43maction_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m state_val = \u001b[38;5;28mself\u001b[39m.critic(state)\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# Store experience in buffer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/distributions/categorical.py:140\u001b[39m, in \u001b[36mCategorical.log_prob\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._validate_args:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_sample(value)\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.unsqueeze(-\u001b[32m1\u001b[39m)\n\u001b[32m    141\u001b[39m value, log_pmf = torch.broadcast_tensors(value, \u001b[38;5;28mself\u001b[39m.logits)\n\u001b[32m    142\u001b[39m value = value[..., :\u001b[32m1\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Proximal Policy Optimization (PPO) Implementation in PyTorch\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "\n",
    "# Set device to cpu or cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    Buffer to store experiences collected during rollouts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.logprobs = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the buffer.\"\"\"\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "        self.logprobs.clear()\n",
    "        self.state_values.clear()\n",
    "        self.is_terminals.clear()\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor network for continuous or discrete action spaces.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64, \n",
    "                 continuous: bool = False, action_std_init: float = 0.6):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.continuous = continuous\n",
    "        \n",
    "        # Common layers\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output layers based on action space type\n",
    "        if continuous:\n",
    "            self.mean_layer = nn.Linear(hidden_dim, action_dim)\n",
    "            self.action_std = nn.Parameter(torch.full((action_dim,), action_std_init))\n",
    "        else:\n",
    "            self.categorical = nn.Linear(hidden_dim, action_dim)\n",
    "            \n",
    "    def forward(self, state: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        if self.continuous:\n",
    "            action_mean = self.mean_layer(x)\n",
    "            action_std = self.action_std.expand_as(action_mean)\n",
    "            return action_mean, action_std\n",
    "        else:\n",
    "            action_probs = torch.softmax(self.categorical(x), dim=-1)\n",
    "            return action_probs\n",
    "    \n",
    "    def set_action_std(self, new_action_std: float):\n",
    "        \"\"\"Set the action standard deviation (for continuous action spaces).\"\"\"\n",
    "        if self.continuous:\n",
    "            self.action_std = nn.Parameter(torch.full(self.action_std.shape, new_action_std))\n",
    "        else:\n",
    "            print(\"WARNING: Calling set_action_std on discrete action space policy\")\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic network that estimates the value of a state.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, hidden_dim: int = 64):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        value = self.value(x)\n",
    "        \n",
    "        return value\n",
    "\n",
    "class PPO:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm with clipped objective.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim: int, \n",
    "        action_dim: int,\n",
    "        lr_actor: float = 3e-4,\n",
    "        lr_critic: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        K_epochs: int = 10,\n",
    "        eps_clip: float = 0.2,\n",
    "        has_continuous_action_space: bool = False,\n",
    "        action_std_init: float = 0.6,\n",
    "        hidden_dim: int = 64\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the PPO agent.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Dimension of action space\n",
    "            lr_actor: Learning rate for actor network\n",
    "            lr_critic: Learning rate for critic network\n",
    "            gamma: Discount factor\n",
    "            K_epochs: Number of epochs to update policy\n",
    "            eps_clip: Clip parameter for PPO\n",
    "            has_continuous_action_space: Whether the action space is continuous\n",
    "            action_std_init: Initial standard deviation for action distribution\n",
    "            hidden_dim: Hidden dimension size for neural networks\n",
    "        \"\"\"\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        # Initialize buffer\n",
    "        self.buffer = RolloutBuffer()\n",
    "        \n",
    "        # Initialize actor\n",
    "        self.actor = ActorNetwork(\n",
    "            state_dim, \n",
    "            action_dim, \n",
    "            hidden_dim=hidden_dim, \n",
    "            continuous=has_continuous_action_space, \n",
    "            action_std_init=action_std_init\n",
    "        ).to(device)\n",
    "        \n",
    "        # Initialize critic\n",
    "        self.critic = CriticNetwork(state_dim, hidden_dim=hidden_dim).to(device)\n",
    "        \n",
    "        # Set optimizers for both networks\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        \n",
    "        # Initialize old policy (for importance sampling)\n",
    "        self.actor_old = ActorNetwork(\n",
    "            state_dim, \n",
    "            action_dim, \n",
    "            hidden_dim=hidden_dim, \n",
    "            continuous=has_continuous_action_space, \n",
    "            action_std_init=action_std_init\n",
    "        ).to(device)\n",
    "        \n",
    "        # Copy parameters from current policy to old policy\n",
    "        self.actor_old.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        # Loss function for value network\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        \n",
    "        # Set initial action standard deviation\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "    \n",
    "    def set_action_std(self, new_action_std: float):\n",
    "        \"\"\"\n",
    "        Set the action standard deviation for continuous action spaces.\n",
    "        \n",
    "        Args:\n",
    "            new_action_std: New standard deviation value\n",
    "        \"\"\"\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.actor.set_action_std(new_action_std)\n",
    "            self.actor_old.set_action_std(new_action_std)\n",
    "        else:\n",
    "            print(\"WARNING: Calling set_action_std on discrete action space policy\")\n",
    "    \n",
    "    def decay_action_std(self, action_std_decay_rate: float, min_action_std: float):\n",
    "        \"\"\"\n",
    "        Decay the action standard deviation based on the decay rate.\n",
    "        \n",
    "        Args:\n",
    "            action_std_decay_rate: Rate at which to decay the std\n",
    "            min_action_std: Minimum std value\n",
    "        \"\"\"\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = max(self.action_std, min_action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "            print(f\"Setting actor output action_std to: {self.action_std}\")\n",
    "        else:\n",
    "            print(\"WARNING: Calling decay_action_std on discrete action space policy\")\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Select an action based on the current policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state observation\n",
    "            \n",
    "        Returns:\n",
    "            Selected action\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            \n",
    "            if self.has_continuous_action_space:\n",
    "                action_mean, action_std = self.actor_old(state)\n",
    "                action_dist = Normal(action_mean, action_std)\n",
    "            else:\n",
    "                action_probs = self.actor_old(state)\n",
    "                action_dist = Categorical(action_probs)\n",
    "            \n",
    "            action = action_dist.sample()\n",
    "            action_logprob = action_dist.log_prob(action)\n",
    "            state_val = self.critic(state)\n",
    "            \n",
    "            # Store experience in buffer\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "            \n",
    "            if self.has_continuous_action_space:\n",
    "                return action.detach().cpu().numpy().flatten()\n",
    "            else:\n",
    "                return action.item()\n",
    "    \n",
    "    def evaluate(self, states: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions given states using the current policy.\n",
    "        \n",
    "        Args:\n",
    "            states: Batch of states\n",
    "            actions: Batch of actions\n",
    "            \n",
    "        Returns:\n",
    "            log_probs, state_values, dist_entropy\n",
    "        \"\"\"\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean, action_std = self.actor(states)\n",
    "            action_dist = Normal(action_mean, action_std)\n",
    "            \n",
    "            # For continuous action spaces, calculate the log probability of each action dimension\n",
    "            if actions.dim() > action_mean.dim():\n",
    "                actions = actions.squeeze(-1)\n",
    "            \n",
    "            action_log_probs = action_dist.log_prob(actions).sum(dim=1)\n",
    "            dist_entropy = action_dist.entropy().sum(dim=1).mean()\n",
    "            \n",
    "        else:\n",
    "            action_probs = self.actor(states)\n",
    "            action_dist = Categorical(action_probs)\n",
    "            \n",
    "            action_log_probs = action_dist.log_prob(actions)\n",
    "            dist_entropy = action_dist.entropy().mean()\n",
    "        \n",
    "        state_values = self.critic(states)\n",
    "        \n",
    "        return action_log_probs, state_values, dist_entropy\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Update policy and value networks using PPO algorithm.\n",
    "        \"\"\"\n",
    "        # Calculate discounted rewards and advantages\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        \n",
    "        # Compute discounted rewards\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        old_states = torch.stack(self.buffer.states, dim=0).detach().to(device)\n",
    "        old_actions = torch.stack(self.buffer.actions, dim=0).detach().to(device)\n",
    "        old_logprobs = torch.stack(self.buffer.logprobs, dim=0).detach().to(device)\n",
    "        old_state_values = torch.stack(self.buffer.state_values, dim=0).squeeze().detach().to(device)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = rewards - old_state_values\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-7)\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluate old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Match dimensions with rewards\n",
    "            state_values = state_values.squeeze()\n",
    "            \n",
    "            # Calculate ratios for importance sampling\n",
    "            ratios = torch.exp(logprobs - old_logprobs)\n",
    "            \n",
    "            # Calculate surrogate losses\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            \n",
    "            # PPO clipped objective with value function loss and entropy bonus\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = self.MseLoss(state_values, rewards)\n",
    "            entropy_loss = -0.01 * dist_entropy  # Entropy bonus for exploration\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = actor_loss + 0.5 * critic_loss + entropy_loss\n",
    "            \n",
    "            # Take gradient step\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "        \n",
    "        # Update old policy\n",
    "        self.actor_old.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        # Clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    def save(self, checkpoint_path: str):\n",
    "        \"\"\"\n",
    "        Save model parameters.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to save the model\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'actor_old': self.actor_old.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "    \n",
    "    def load(self, checkpoint_path: str):\n",
    "        \"\"\"\n",
    "        Load model parameters.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to load the model from\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.actor_old.load_state_dict(checkpoint['actor_old'])\n",
    "\n",
    "def train(env_name: str, max_episodes: int = 1000, max_timesteps: int = 1000, \n",
    "          update_timestep: int = 4000, log_interval: int = 20, save_interval: int = 100,\n",
    "          has_continuous_action_space: bool = False, action_std_init: float = 0.6) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train a PPO agent in a given environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Name of the gym environment\n",
    "        max_episodes: Maximum number of episodes for training\n",
    "        max_timesteps: Maximum timesteps in one episode\n",
    "        update_timestep: Update policy every n timesteps\n",
    "        log_interval: Print avg reward in the interval\n",
    "        save_interval: Save model in the interval\n",
    "        has_continuous_action_space: Whether the environment has continuous action space\n",
    "        action_std_init: Initial action standard deviation\n",
    "    \n",
    "    Returns:\n",
    "        List of average rewards per episode\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    \n",
    "    if has_continuous_action_space:\n",
    "        action_dim = env.action_space.shape[0]\n",
    "    else:\n",
    "        action_dim = env.action_space.n\n",
    "    \n",
    "    # Initialize PPO agent\n",
    "    ppo_agent = PPO(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        lr_actor=3e-4,\n",
    "        lr_critic=1e-3,\n",
    "        gamma=0.99,\n",
    "        K_epochs=10,\n",
    "        eps_clip=0.2,\n",
    "        has_continuous_action_space=has_continuous_action_space,\n",
    "        action_std_init=action_std_init\n",
    "    )\n",
    "    \n",
    "    # Logging variables\n",
    "    running_reward = 0\n",
    "    avg_length = 0\n",
    "    time_step = 0\n",
    "    \n",
    "    # Training loop\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(1, max_episodes+1):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in range(1, max_timesteps+1):\n",
    "            # Select action\n",
    "            action = ppo_agent.select_action(state)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Store experience in buffer\n",
    "            ppo_agent.buffer.rewards.append(reward)\n",
    "            ppo_agent.buffer.is_terminals.append(done)\n",
    "            \n",
    "            time_step += 1\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Update if its time\n",
    "            if time_step % update_timestep == 0:\n",
    "                ppo_agent.update()\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "        episode_rewards.append(running_reward)\n",
    "        \n",
    "        # Logging\n",
    "        if episode % log_interval == 0:\n",
    "            print(f\"Episode: {episode}, Avg. Reward: {running_reward:.2f}\")\n",
    "        \n",
    "        # Save model\n",
    "        if episode % save_interval == 0:\n",
    "            ppo_agent.save(f\"./PPO_{env_name}_{episode}.pth\")\n",
    "    \n",
    "    env.close()\n",
    "    return episode_rewards\n",
    "\n",
    "def plot_learning_curve(rewards: List[float], title: str = \"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    Plot the learning curve based on rewards.\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rewards)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{title.replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "\n",
    "rewards_discrete = train(\n",
    "    env_name=\"CartPole-v1\",\n",
    "    max_episodes=5000,\n",
    "    max_timesteps=1000,\n",
    "    has_continuous_action_space=False,\n",
    "    log_interval=100,\n",
    ")\n",
    "# plot_learning_curve(rewards_discrete, \"CartPole-v1 Learning Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent = PPO(actor, critic, actor_old, ppo_config)\n",
    "ppo_agent.load(\"./PPO_RLHFEnv-v0_2.pth\")\n",
    "ppo_agent.actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curve(rewards: List[float], title: str = \"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    Plot the learning curve based on rewards.\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rewards)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{title.replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curve(rewards_discrete, \"CartPole-v1 Learning Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For continuous action space (Pendulum-v1)\n",
    "rewards_continuous = train(\n",
    "    env_name=\"Pendulum-v1\",\n",
    "    max_episodes=1000,\n",
    "    max_timesteps=1000,\n",
    "    has_continuous_action_space=True,\n",
    "    action_std_init=0.6\n",
    ")\n",
    "plot_learning_curve(rewards_continuous, \"Pendulum-v1 Learning Curve\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning-lm-from-human-preferences-4SAAosyV-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
