{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# MODEL_NAME = 'openai-community/gpt2-large' TODO: change to gpt2-large\n",
    "MODEL_NAME = 'openai-community/gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  2, -1, -1],\n",
      "        [ 0,  1,  2, -1, -1],\n",
      "        [ 0,  2,  5,  6,  7],\n",
      "        [ 0,  3,  5,  6,  7],\n",
      "        [ 1,  0, -1, -1, -1],\n",
      "        [ 1,  1, -1, -1, -1],\n",
      "        [ 1,  2,  2, -1, -1],\n",
      "        [ 1,  3,  2, -1, -1],\n",
      "        [ 2,  0,  5,  6,  7],\n",
      "        [ 2,  1,  5,  6,  7],\n",
      "        [ 2,  2, -1, -1, -1],\n",
      "        [ 2,  3, -1, -1, -1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0,  2, -1, -1],\n",
       "         [ 0,  1,  2, -1, -1],\n",
       "         [ 0,  2,  5,  6,  7],\n",
       "         [ 0,  3,  5,  6,  7]],\n",
       "\n",
       "        [[ 1,  0, -1, -1, -1],\n",
       "         [ 1,  1, -1, -1, -1],\n",
       "         [ 1,  2,  2, -1, -1],\n",
       "         [ 1,  3,  2, -1, -1]],\n",
       "\n",
       "        [[ 2,  0,  5,  6,  7],\n",
       "         [ 2,  1,  5,  6,  7],\n",
       "         [ 2,  2, -1, -1, -1],\n",
       "         [ 2,  3, -1, -1, -1]]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "arr = [\n",
    "    torch.tensor([0, 0, 2]),\n",
    "    torch.tensor([0, 1, 2]),\n",
    "    torch.tensor([0, 2, 5, 6, 7]),\n",
    "    torch.tensor([0, 3, 5, 6, 7]),\n",
    "    \n",
    "    torch.tensor([1, 0]),\n",
    "    torch.tensor([1, 1]),\n",
    "    torch.tensor([1, 2, 2]),\n",
    "    torch.tensor([1, 3, 2]),\n",
    "    \n",
    "    torch.tensor([2, 0, 5, 6, 7]),\n",
    "    torch.tensor([2, 1, 5, 6, 7]),\n",
    "    torch.tensor([2, 2]),\n",
    "    torch.tensor([2, 3])\n",
    "]\n",
    "result = pad_sequence(arr, batch_first=True, padding_value=-1)\n",
    "print(result)\n",
    "result = result.view(3, 4, -1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      " tensor([[  464,  2068,  7586, 21831, 18045,   625,   262, 16931,  3290,    13,\n",
      "         50256],\n",
      "        [41762,   364,   389,   257,  3665,  2891,   329,  3288,  3303,  7587,\n",
      "            13]])\n",
      "Attention Mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([2, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Transformers are a powerful tool for natural language processing.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Input IDs:\\n\", inputs['input_ids'])\n",
    "print(\"Attention Mask:\\n\", inputs['attention_mask'])\n",
    "\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "last_hidden_state = outputs.hidden_states[-1]\n",
    "print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  4,  9, 16, 25])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3, 4, 5]) * torch.tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[8727,  389,  345]]), 'attention_mask': tensor([[1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'who are you?\"\\n\\n\"I\\'m not sure. I\\'m not sure if I\\'m going to be able'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer(['who are you'], return_tensors=\"pt\")\n",
    "print(encoded)\n",
    "encoded = model.generate(**encoded)\n",
    "tokenizer.decode(encoded[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31373, 38744,     0, 50256, 50256,    40,   716,   220,  1924,   707,\n",
      "          8727,   716,   314, 50256, 50256]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello folds!<|endoftext|><|endoftext|>I am eryawwho am I<|endoftext|><|endoftext|>\\n\\nI am eryawwho am I\\n\\nI am eryawwho am I'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"hello folds!\",\n",
    "    \"I am eryaw\",\n",
    "    \"who am I\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "input_ids = torch.cat([inputs['input_ids'][0], inputs['input_ids'][1], inputs['input_ids'][2]], dim=0).unsqueeze(dim=0)\n",
    "attention_mask = torch.cat([inputs['attention_mask'][0], inputs['attention_mask'][1], inputs['attention_mask'][2]], dim=0).unsqueeze(dim=0)\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "output_encoded = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "tokenizer.decode(output_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _combine_query_sample(tokenizer_pad, query: torch.Tensor, sample: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            query: torch.tensor, shape (batch_size, seq_len)\n",
    "            sample: torch.tensor, shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            combined: torch.tensor, shape (batch_size, seq_len)\n",
    "        \n",
    "        Combine query and samples prompt, remove pad between query and samples\n",
    "        Query format is [Q1, Q2, ..., PAD, PAD]\n",
    "        Sample format is [S1, S2, ..., PAD, PAD]\n",
    "        Output format is [Q1, Q2, ..., S1, S2, ..., PAD, PAD]\n",
    "    \"\"\"\n",
    "    assert query.dim() == 1 and sample.dim() == 1, 'Query and Sample must be 1D tensor'\n",
    "    combined_query_sample = torch.full((query.size(0) + sample.size(0),), tokenizer_pad)\n",
    "    # remove pad tokens on query\n",
    "    new_query = query[query != tokenizer_pad] \n",
    "    combined_query_sample[:new_query.size(0)] = new_query\n",
    "    \n",
    "    combined_query_sample[new_query.size(0):new_query.size(0)+sample.size(0)] = sample\n",
    "    return combined_query_sample\n",
    "    \n",
    "x = torch.tensor([1, 2, 3, 0, 0])\n",
    "y = torch.tensor([4, 5, 0, 0, 0])\n",
    "\n",
    "_combine_query_sample(0, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[31373,   220,   220,   220]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "text = ['hello   ']\n",
    "input_tokens = tokenizer(text, return_tensors='pt')\n",
    "print(input_tokens)\n",
    "# torch.masked_fill(input_tokens)\n",
    "# model(**input_tokens, attention_mask=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tsr = torch.tensor([\u001b[32m31373\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtsr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fine_tuning_from_human_references/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fine_tuning_from_human_references/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fine_tuning_from_human_references/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fine_tuning_from_human_references/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fine_tuning_from_human_references/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fine_tuning_from_human_references/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:794\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwarn_if_padding_and_no_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m     input_shape = input_ids.size()\n\u001b[32m    796\u001b[39m     input_ids = input_ids.view(-\u001b[32m1\u001b[39m, input_shape[-\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fine_tuning_from_human_references/lib/python3.11/site-packages/transformers/modeling_utils.py:5153\u001b[39m, in \u001b[36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m   5150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   5152\u001b[39m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.pad_token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[32m   5154\u001b[39m     warn_string = (\n\u001b[32m   5155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5156\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5158\u001b[39m     )\n\u001b[32m   5160\u001b[39m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[32m   5161\u001b[39m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "tsr = torch.tensor([31373])\n",
    "model(input_ids=tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_human_preferences import train_reward\n",
    "\n",
    "reward_data1 = train_reward.RewardData.from_openai('../data/descriptiveness_offline_5k.json').to_dataset().select(range(10))\n",
    "reward_data2 = train_reward.RewardData.from_openai('../data/descriptiveness_offline_5k.json').to_dataset().select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of reward_data1: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Mapped reward data: Dataset({\n",
      "    features: ['query', 'sample0', 'sample1', 'sample2', 'sample3', 'best'],\n",
      "    num_rows: 6260\n",
      "})\n",
      "First query tensor: tensor([6.5420e+03, 3.1140e+03, 8.6600e+02, 1.1000e+01, 2.9000e+02, 2.4970e+03,\n",
      "        6.0600e+02, 3.3930e+03, 1.3000e+01, 6.7900e+02, 6.3500e+02, 2.4970e+03,\n",
      "        4.8400e+02, 5.4700e+02, 2.0450e+03, 3.2640e+03, 7.3600e+02, 3.7900e+02,\n",
      "        6.8300e+02, 1.3000e+01, 1.3180e+03, 3.7300e+02, 2.5700e+02, 5.8200e+02,\n",
      "        3.5100e+02, 2.5700e+02, 1.3360e+03, 2.1213e+04, 1.1000e+01, 1.1940e+03,\n",
      "        5.8200e+02, 2.1804e+04, 3.1900e+02, 2.5700e+02, 3.3009e+04, 1.1000e+01,\n",
      "        2.9000e+02, 2.5700e+02, 6.2830e+03, 1.2000e+01, 1.1534e+04, 4.2700e+02,\n",
      "        3.6300e+02, 1.3600e+03, 2.3300e+03, 1.2000e+01, 3.9200e+02, 1.2000e+01,\n",
      "        3.3282e+04, 3.2900e+03, 3.2600e+02, 3.7300e+02, 4.9880e+03, 1.3110e+03,\n",
      "        5.0600e+02, 5.1600e+02, 2.8700e+02, 2.5460e+03, 1.3000e+01, 5.0259e+04,\n",
      "        5.0259e+04, 5.0259e+04, 5.0259e+04, 5.0259e+04])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def map_reward_to_tensor(batch):\n",
    "    tensor_batch = {\n",
    "        key: torch.tensor(batch[key], dtype=torch.float32)\n",
    "        for key in batch.keys()\n",
    "    }\n",
    "    assert isinstance(tensor_batch['query'], torch.Tensor)\n",
    "    return tensor_batch\n",
    "\n",
    "# Ensure reward_data1 is a dataset that supports the map method\n",
    "print(\"Type of reward_data1:\", type(reward_data1))  # Debugging print statement\n",
    "\n",
    "reward_data1.set_transform(map_reward_to_tensor)\n",
    "print(\"Mapped reward data:\", reward_data1)  # Debugging print statement\n",
    "\n",
    "# Accessing the first query to check the result\n",
    "print(\"First query tensor:\", reward_data1['query'][0])  # Debugging print statement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine_tuning_from_human_references",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
