{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration with PyTorchRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ez/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ez/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torchrl/data/replay_buffers/samplers.py:34: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. This is likely due to a discrepancy between your package version and the PyTorch version. Make sure both are compatible. Usually, torchrl majors follow the pytorch majors within a few days around the release. For instance, TorchRL 0.5 requires PyTorch 2.4.0, and TorchRL 0.6 requires PyTorch 2.5.0.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from lm_human_preferences.env.rlhf_env import RLHFEnv\n",
    "from lm_human_preferences.data.base import QueryData\n",
    "from lm_human_preferences.lm.reward import RewardModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from torchrl.envs import GymEnv\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "def global_init_env():\n",
    "    gym.envs.register(\n",
    "        id='RLHFEnv-v0',\n",
    "        entry_point='lm_human_preferences.env.rlhf_env:RLHFEnv',\n",
    "        kwargs={\n",
    "            'ref_model_name': 'openai-community/gpt2',\n",
    "            'reward_model': RewardModel.from_pretrained('../models/reward_model'),\n",
    "            'dataset': QueryData.from_openai_format(\n",
    "                AutoTokenizer.from_pretrained('openai-community/gpt2'),\n",
    "                '../data/descriptiveness_offline_5k'\n",
    "            ),\n",
    "            'kl_coef': 0.01,\n",
    "            'max_generation': 64,\n",
    "            'device': DEVICE,\n",
    "            'seed': 42\n",
    "        }\n",
    "    )\n",
    "\n",
    "global_init_env()\n",
    "base_env = GymEnv(\"RLHFEnv-v0\", categorical_action_encoding=True, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([54, 50257]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([54, 50257]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([55, 50257]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([54, 50257]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "tensordict = base_env.reset()\n",
    "print(tensordict)\n",
    "tensordict_with_action = base_env.rand_action(tensordict)\n",
    "print(tensordict_with_action)\n",
    "step_tensordict = base_env.step(tensordict_with_action)\n",
    "print(step_tensordict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor(30212, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(tensordict_with_action['action']) # YOU HAVE TO KNOW THAT THIS OUTPUT SPACE ITSELF RATHER THAN ID\n",
    "print(tensordict_with_action['action'].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([55, 50257]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import step_mdp\n",
    "\n",
    "# Move and replace all next: tensordict items into root keys\n",
    "data = step_mdp(step_tensordict)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LazyStackedTensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 50257]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "        next: LazyStackedTensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "                observation: Tensor(shape=torch.Size([10, -1, 50257]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
       "                reward: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "                terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "                truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
       "            exclusive_fields={\n",
       "            },\n",
       "            batch_size=torch.Size([10]),\n",
       "            device=cuda:0,\n",
       "            is_shared=True,\n",
       "            stack_dim=0),\n",
       "        observation: Tensor(shape=torch.Size([10, -1, 50257]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
       "    exclusive_fields={\n",
       "    },\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cuda:0,\n",
       "    is_shared=True,\n",
       "    stack_dim=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_env.rollout(max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP : 0\n",
      " Hun and Sweetie, the gray-haired ancient waitresses that owed this place during daylight hours shuffled around, bringing everyone sodas and water. And Sal, Pete's business partner and the daytime cook, stepped out with plates and plates of food. Everybody was celebrating.\n",
      "STEP : 1\n",
      " Hun and Sweetie, the gray-haired ancient waitresses that owed this place during daylight hours shuffled around, bringing everyone sodas and water. And Sal, Pete's business partner and the daytime cook, stepped out with plates and plates of food. Everybody was celebrating.olic\n",
      "STEP : 2\n",
      " Hun and Sweetie, the gray-haired ancient waitresses that owed this place during daylight hours shuffled around, bringing everyone sodas and water. And Sal, Pete's business partner and the daytime cook, stepped out with plates and plates of food. Everybody was celebrating.olicrf\n",
      "STEP : 3\n",
      " Hun and Sweetie, the gray-haired ancient waitresses that owed this place during daylight hours shuffled around, bringing everyone sodas and water. And Sal, Pete's business partner and the daytime cook, stepped out with plates and plates of food. Everybody was celebrating.olicrf hypot\n",
      "STEP : 4\n",
      " Hun and Sweetie, the gray-haired ancient waitresses that owed this place during daylight hours shuffled around, bringing everyone sodas and water. And Sal, Pete's business partner and the daytime cook, stepped out with plates and plates of food. Everybody was celebrating.olicrf hypotasuring\n",
      "STEP : 5\n",
      " Hun and Sweetie, the gray-haired ancient waitresses that owed this place during daylight hours shuffled around, bringing everyone sodas and water. And Sal, Pete's business partner and the daytime cook, stepped out with plates and plates of food. Everybody was celebrating.olicrf hypotasuringagle\n",
      "STEP : 6\n",
      " Hun and Sweetie, the gray-haired ancient waitresses that owed this place during daylight hours shuffled around, bringing everyone sodas and water. And Sal, Pete's business partner and the daytime cook, stepped out with plates and plates of food. Everybody was celebrating.olicrf hypotasuringagle normally\n",
      "STEP : 7\n",
      " Hun and Sweetie, the gray-haired ancient waitresses that owed this place during daylight hours shuffled around, bringing everyone sodas and water. And Sal, Pete's business partner and the daytime cook, stepped out with plates and plates of food. Everybody was celebrating.olicrf hypotasuringagle normallywy\n",
      "STEP : 8\n",
      " Hun and Sweetie, the gray-haired ancient waitresses that owed this place during daylight hours shuffled around, bringing everyone sodas and water. And Sal, Pete's business partner and the daytime cook, stepped out with plates and plates of food. Everybody was celebrating.olicrf hypotasuringagle normallywy invites\n",
      "STEP : 9\n",
      " Hun and Sweetie, the gray-haired ancient waitresses that owed this place during daylight hours shuffled around, bringing everyone sodas and water. And Sal, Pete's business partner and the daytime cook, stepped out with plates and plates of food. Everybody was celebrating.olicrf hypotasuringagle normallywy invites disclosing\n"
     ]
    }
   ],
   "source": [
    "rollout = base_env.rollout(max_steps=10)\n",
    "tokenizer = base_env.env.tokenizer\n",
    "pad_token = tokenizer.pad_token_id\n",
    "obs_rollout = rollout.get('observation', as_padded_tensor=True, padding_side='left', padding_value=pad_token)\n",
    "obs_rollout.shape # steps, seq_len, vocab_size\n",
    "for inx, step in enumerate(rollout):\n",
    "    obs = step.get('observation', as_padded_tensor=True, padding_side='left', padding_value=pad_token)\n",
    "    ids = obs.argmax(dim=1)\n",
    "    print(f\"STEP : {inx}\")\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 15:49:01,082 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs.utils import check_env_specs\n",
    "\n",
    "check_env_specs(base_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: Composite(\n",
      "    observation: OneHot(\n",
      "        shape=torch.Size([-1, 50257]),\n",
      "        space=CategoricalBox(n=50257),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.int64,\n",
      "        domain=discrete),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuous(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "    device=cuda:0,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n",
      "input_spec: Composite(\n",
      "    full_state_spec: Composite(\n",
      "    ,\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([])),\n",
      "    full_action_spec: Composite(\n",
      "        action: OneHot(\n",
      "            shape=torch.Size([50257]),\n",
      "            space=CategoricalBox(n=50257),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([])),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([]))\n",
      "action_spec (as defined by input_spec): OneHot(\n",
      "    shape=torch.Size([50257]),\n",
      "    space=CategoricalBox(n=50257),\n",
      "    device=cuda:0,\n",
      "    dtype=torch.int64,\n",
      "    domain=discrete)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", base_env.observation_spec)\n",
    "print(\"reward_spec:\", base_env.reward_spec)\n",
    "print(\"input_spec:\", base_env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", base_env.action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Policy and Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ez/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ez/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torchrl/data/replay_buffers/samplers.py:34: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. This is likely due to a discrepancy between your package version and the PyTorch version. Make sure both are compatible. Usually, torchrl majors follow the pytorch majors within a few days around the release. For instance, TorchRL 0.5 requires PyTorch 2.4.0, and TorchRL 0.6 requires PyTorch 2.5.0.\n",
      "  warnings.warn(EXTENSION_WARNING)\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from lm_human_preferences.lm.reward import RewardModel\n",
    "from torchrl.envs import GymEnv\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from lm_human_preferences.env.rlhf_env import RLHFEnv\n",
    "from lm_human_preferences.data.base import QueryData\n",
    "from lm_human_preferences.lm.reward import RewardModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = 'openai-community/gpt2'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1).to(DEVICE)\n",
    "value_model.transformer.requires_grad_(False)\n",
    "value_model.score.requires_grad_(True)\n",
    "value_model.config.pad_token_id = value_model.config.eos_token_id\n",
    "reward_model = RewardModel.from_pretrained(\"../models/reward_model\")\n",
    "\n",
    "def global_init_env():\n",
    "    gym.envs.register(\n",
    "        id='RLHFEnv-v0',\n",
    "        entry_point='lm_human_preferences.env.rlhf_env:RLHFEnv',\n",
    "        kwargs={\n",
    "            'ref_model_name': 'openai-community/gpt2',\n",
    "            'reward_model': reward_model,\n",
    "            'dataset': QueryData.from_openai_format(\n",
    "                AutoTokenizer.from_pretrained('openai-community/gpt2'),\n",
    "                '../data/descriptiveness_offline_5k'\n",
    "            ),\n",
    "            'kl_coef': 0.01,\n",
    "            'max_generation': 64,\n",
    "            'device': DEVICE,\n",
    "            'seed': 42\n",
    "        }\n",
    "    )\n",
    "\n",
    "global_init_env()\n",
    "base_env = GymEnv(\"RLHFEnv-v0\", device=DEVICE)\n",
    "\n",
    "torchdict = base_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPolicy(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # input_ids [batch_size, seq_len, vocab_size]\n",
    "        if input_ids.ndim == 2:\n",
    "            input_ids = input_ids.unsqueeze(dim=0)\n",
    "            \n",
    "        input_ids = input_ids.argmax(dim=-1) # [batch_size, seq_len]\n",
    "        attention_mask = input_ids != tokenizer.pad_token_id\n",
    "        logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits # [batch_size, seq_len, vocab_size]\n",
    "        return logits[:, -1, :] # [batch_size, vocab_size]\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.modules import ProbabilisticActor\n",
    "from torchrl.modules.distributions.discrete import OneHotCategorical\n",
    "\n",
    "policy_module = TensorDictModule(\n",
    "    module=TransformerPolicy(model),\n",
    "    in_keys=[\"observation\"],   # what the env will supply in the tensordict\n",
    "    out_keys=[\"logits\"],      # what we'll hand to Categorical\n",
    ")\n",
    "\n",
    "# dist_module = CategoricalParamWrapper(logits_key=\"logits\", dist_key=\"dist\")\n",
    "actor = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    in_keys=[\"logits\"],\n",
    "    out_keys=[\"action\"],\n",
    "    distribution_class=OneHotCategorical,     # ← use Categorical\n",
    "    spec=base_env.action_spec,          # your Discrete(vocab_size)\n",
    "    return_log_prob=True\n",
    ")\n",
    "\n",
    "torchdict = actor(torchdict)\n",
    "# print(torchdict['logits'])\n",
    "# torchdict['action'].argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import ValueOperator\n",
    "\n",
    "class TransformerValue(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # input_ids [batch_size, seq_len, vocab_size]\n",
    "        if input_ids.ndim == 2:\n",
    "            input_ids = input_ids.unsqueeze(dim=0)\n",
    "    \n",
    "        input_ids = input_ids.argmax(dim=-1)\n",
    "        attention_mask = input_ids != tokenizer.pad_token_id\n",
    "        logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits # [batch_size, 1]\n",
    "        \n",
    "        return logits[:, -1]\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=TransformerValue(value_model),\n",
    "    in_keys=[\"observation\"]\n",
    ")\n",
    "# torchdict = value_module(torchdict)\n",
    "# torchdict['state_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers import LazyTensorStorage, ReplayBuffer, SamplerWithoutReplacement\n",
    "\n",
    "# This is a sequential collector\n",
    "collector = SyncDataCollector(\n",
    "    base_env,\n",
    "    actor,\n",
    "    frames_per_batch=5,\n",
    "    total_frames=15,\n",
    "    split_trajs=False,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=30),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")\n",
    "\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "\n",
    "advantage_module = GAE(\n",
    "    gamma=.99, lmbda=1, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=0.2,\n",
    "    entropy_bonus=True,\n",
    "    entropy_coef=0.01,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, 3000 // 30, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_observation_and_next_observation(obs_data, next_obs_data):\n",
    "    token_ids = obs_data.argmax(dim=-1) # (step, seq_len)\n",
    "    token_ids_mask = obs_data.sum(dim=-1) == 0\n",
    "    token_ids[token_ids_mask] = tokenizer.pad_token_id\n",
    "\n",
    "    next_token_ids = next_obs_data.argmax(dim=-1)\n",
    "    next_token_ids_mask = next_obs_data.sum(dim=-1) == 0\n",
    "    next_token_ids[next_token_ids_mask] = tokenizer.pad_token_id\n",
    "\n",
    "    max_seq_len = max(token_ids.size(1), next_token_ids.size(1))\n",
    "\n",
    "    # Pad token_ids and next_token_ids to the same sequence length\n",
    "    if token_ids.size(1) < max_seq_len:\n",
    "        token_ids = F.pad(token_ids, (0, max_seq_len - token_ids.size(1)), value=tokenizer.pad_token_id)\n",
    "    if next_token_ids.size(1) < max_seq_len:\n",
    "        next_token_ids = F.pad(next_token_ids, (0, max_seq_len - next_token_ids.size(1)), value=tokenizer.pad_token_id)\n",
    "\n",
    "    one_hot = F.one_hot(token_ids, num_classes=tokenizer.vocab_size)\n",
    "    next_one_hot = F.one_hot(next_token_ids, num_classes=tokenizer.vocab_size)\n",
    "\n",
    "    return one_hot, next_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ez/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torchrl/envs/libs/gym.py:1150: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  action = int(action)\n"
     ]
    }
   ],
   "source": [
    "tensor_data = next(iter(collector))\n",
    "\n",
    "tensor_data['observation'], tensor_data['next']['observation'] = pad_observation_and_next_observation(\n",
    "    tensor_data.get(\n",
    "        'observation',\n",
    "        as_padded_tensor=True,\n",
    "        padding_value=0,\n",
    "        padding_side=\"left\")\n",
    "    ,\n",
    "    tensor_data['next'].get(\n",
    "        'observation',\n",
    "        as_padded_tensor=True,\n",
    "        padding_value=0,\n",
    "        padding_side=\"left\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|endoftext|>  Chris, you too.\"\n",
      "Mrs. Browley threw her husband an impressed look.\n",
      "\"That sounds like a wonderful idea, Priscilla.\"  Mr. Browley stated.  \"Mason, Hanna, Chris?\"\n",
      "\"That sounds good to me.\"  Hanna replied. He was<|endoftext|>\n",
      "<|endoftext|><|endoftext|>  Chris, you too.\"\n",
      "Mrs. Browley threw her husband an impressed look.\n",
      "\"That sounds like a wonderful idea, Priscilla.\"  Mr. Browley stated.  \"Mason, Hanna, Chris?\"\n",
      "\"That sounds good to me.\"  Hanna replied. He was no\n",
      "torch.Size([5, 64, 50257])\n",
      "torch.Size([5, 64, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tensor_data['observation'].argmax(dim=-1)[2]))\n",
    "print(tokenizer.decode(tensor_data['next']['observation'].argmax(dim=-1)[2]))\n",
    "print(tensor_data['observation'].shape)\n",
    "print(tensor_data['next']['observation'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "All input tensors (value, reward and done states) must share a unique shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[218]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m advantage, value_target\n\u001b[32m    101\u001b[39m gae = CustomGAE(gamma=\u001b[32m0.99\u001b[39m, lmbda=\u001b[32m0.95\u001b[39m, normalize_advantages=\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[43mgae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstate_value\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstate_value\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreward\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdone\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mterminated\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/finetuning-lm-from-human-preferences-4SAAosyV-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[218]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mCustomGAE.forward\u001b[39m\u001b[34m(self, state_value, next_state_value, reward, done, terminated)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Check shapes  \u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (next_state_value.shape == state_value.shape == reward.shape == done.shape == terminated.shape):  \n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll input tensors (value, reward and done states) must share a unique shape.\u001b[39m\u001b[33m\"\u001b[39m)  \n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Move constants to the right device  \u001b[39;00m\n\u001b[32m     59\u001b[39m device = state_value.device  \n",
      "\u001b[31mRuntimeError\u001b[39m: All input tensors (value, reward and done states) must share a unique shape."
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "  \n",
    "class CustomGAE(nn.Module):  \n",
    "    \"\"\"Custom Generalized Advantage Estimation module that doesn't use vmap.  \n",
    "      \n",
    "    This implementation follows the algorithm described in the paper   \n",
    "    \"HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION\"  \n",
    "    https://arxiv.org/pdf/1506.02438.pdf  \n",
    "    \"\"\"  \n",
    "      \n",
    "    def __init__(  \n",
    "        self,  \n",
    "        gamma: float,  \n",
    "        lmbda: float,  \n",
    "        normalize_advantages: bool = False  \n",
    "    ):  \n",
    "        \"\"\"Initialize the GAE module.  \n",
    "          \n",
    "        Args:  \n",
    "            gamma (float): Discount factor for future rewards  \n",
    "            lmbda (float): GAE lambda parameter for controlling bias-variance tradeoff  \n",
    "            normalize_advantages (bool): Whether to normalize advantages. Default: False  \n",
    "        \"\"\"  \n",
    "        super().__init__()  \n",
    "        self.register_buffer(\"gamma\", torch.tensor(gamma))  \n",
    "        self.register_buffer(\"lmbda\", torch.tensor(lmbda))  \n",
    "        self.normalize_advantages = normalize_advantages  \n",
    "      \n",
    "    def forward(  \n",
    "        self,  \n",
    "        state_value: torch.Tensor,  \n",
    "        next_state_value: torch.Tensor,  \n",
    "        reward: torch.Tensor,  \n",
    "        done: torch.Tensor,  \n",
    "        terminated: torch.Tensor = None,  \n",
    "    ):  \n",
    "        \"\"\"Compute GAE advantages and value targets.  \n",
    "          \n",
    "        Args:  \n",
    "            state_value: Value estimates of states at time t [batch_size, time_steps, 1]  \n",
    "            next_state_value: Value estimates of states at time t+1 [batch_size, time_steps, 1]  \n",
    "            reward: Rewards received at time t [batch_size, time_steps, 1]  \n",
    "            done: Boolean tensor indicating if episode is done [batch_size, time_steps, 1]  \n",
    "            terminated: Boolean tensor indicating if episode is terminated [batch_size, time_steps, 1]  \n",
    "          \n",
    "        Returns:  \n",
    "            tuple: (advantages, value_targets)  \n",
    "        \"\"\"  \n",
    "        # Handle defaults  \n",
    "        if terminated is None:  \n",
    "            terminated = done.clone()  \n",
    "          \n",
    "        # Check shapes  \n",
    "        if not (next_state_value.shape == state_value.shape == reward.shape == done.shape == terminated.shape):  \n",
    "            raise RuntimeError(\"All input tensors (value, reward and done states) must share a unique shape.\")  \n",
    "          \n",
    "        # Move constants to the right device  \n",
    "        device = state_value.device  \n",
    "        if self.gamma.device != device:  \n",
    "            self.gamma = self.gamma.to(device)  \n",
    "        if self.lmbda.device != device:  \n",
    "            self.lmbda = self.lmbda.to(device)  \n",
    "          \n",
    "        gamma = self.gamma  \n",
    "        lmbda = self.lmbda  \n",
    "          \n",
    "        # Setup  \n",
    "        not_done = (~done).int()  \n",
    "        not_terminated = (~terminated).int()  \n",
    "        *batch_size, time_steps, lastdim = not_done.shape  \n",
    "          \n",
    "        # Preallocate advantage tensor  \n",
    "        advantage = torch.zeros_like(state_value)  \n",
    "          \n",
    "        # Calculate TD error: δ_t = r_t + γV(s_{t+1}) - V(s_t)  \n",
    "        g_not_terminated = gamma * not_terminated  \n",
    "        delta = reward + (g_not_terminated * next_state_value) - state_value  \n",
    "          \n",
    "        # Calculate GAE by backward recursion  \n",
    "        discount = lmbda * gamma * not_done  \n",
    "          \n",
    "        # Initialize to zero for the last timestep  \n",
    "        last_gae = torch.zeros_like(delta[..., 0, :])  \n",
    "          \n",
    "        # Use a single reverse for loop - unavoidable for GAE calculation  \n",
    "        for t in reversed(range(time_steps)):  \n",
    "            # Compute current GAE value  \n",
    "            last_gae = delta[..., t, :] + discount[..., t, :] * last_gae  \n",
    "            advantage[..., t, :] = last_gae  \n",
    "          \n",
    "        # Calculate value targets: V_target = A + V  \n",
    "        value_target = advantage + state_value  \n",
    "          \n",
    "        # Optionally normalize advantages  \n",
    "        if self.normalize_advantages:  \n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)  \n",
    "          \n",
    "        return advantage, value_target\n",
    "\n",
    "gae = CustomGAE(gamma=0.99, lmbda=0.95, normalize_advantages=True)  \n",
    "gae(\n",
    "    tensor_data['state_value'],\n",
    "    tensor_data['next']['state_value'],\n",
    "    tensor_data['next']['reward'],\n",
    "    tensor_data['done'],\n",
    "    tensor_data['terminated']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LazyStackedTensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([30, 50257]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        collector: LazyStackedTensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([30]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            exclusive_fields={\n",
      "            },\n",
      "            batch_size=torch.Size([30]),\n",
      "            device=cpu,\n",
      "            is_shared=False,\n",
      "            stack_dim=0),\n",
      "        done: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        logits: Tensor(shape=torch.Size([30, 50257]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: LazyStackedTensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([30, -1, 50257]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            exclusive_fields={\n",
      "            },\n",
      "            batch_size=torch.Size([30]),\n",
      "            device=cpu,\n",
      "            is_shared=False,\n",
      "            stack_dim=0),\n",
      "        observation: Tensor(shape=torch.Size([30, -1, 50257]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    exclusive_fields={\n",
      "    },\n",
      "    batch_size=torch.Size([30]),\n",
      "    device=cpu,\n",
      "    is_shared=False,\n",
      "    stack_dim=0)\n"
     ]
    }
   ],
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Pytorch Only Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning-lm-from-human-preferences-4SAAosyV-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
